import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt


# --- 1. å®šä¹‰é€šä¿¡ç³»ç»Ÿ (Tx + Channel + Rx) ---
class AutoencoderComm(nn.Module):
    def __init__(self, M=16, n_channel=2):
        super(AutoencoderComm, self).__init__()
        self.M = M  # æ¶ˆæ¯ç§ç±»æ•° (æ¯”å¦‚ 16 ç§æ¶ˆæ¯ï¼Œç›¸å½“äº 16-QAM)
        self.n_channel = n_channel  # ä¿¡é“ç»´åº¦ (2 ä»£è¡¨å®éƒ¨å’Œè™šéƒ¨ï¼Œå³ 1ä¸ªå¤æ•°ç¬¦å·)

        # --- å‘å°„æœº (Transmitter) ---
        # è¾“å…¥: Mç»´ one-hot å‘é‡ -> è¾“å‡º: 2ç»´ (I, Q)
        self.transmitter = nn.Sequential(
            nn.Linear(M, 32),
            nn.ReLU(),
            nn.Linear(32, n_channel)
            # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰ Tanh æˆ– Sigmoidï¼Œæˆ‘ä»¬å…è®¸ AI å‘å°„ä»»æ„èƒ½é‡çš„ä¿¡å·
            # ä½†æˆ‘ä»¬éœ€è¦åœ¨ forward é‡Œæ‰‹åŠ¨åšå½’ä¸€åŒ–
        )

        # --- æ¥æ”¶æœº (Receiver) ---
        # è¾“å…¥: 2ç»´ (I, Q) + å™ªå£° -> è¾“å‡º: Mç»´ (æ¦‚ç‡)
        self.receiver = nn.Sequential(
            nn.Linear(n_channel, 32),
            nn.ReLU(),
            nn.Linear(32, M),
            nn.Softmax(dim=1)  # è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
        )

    def forward(self, x, noise_std=0.1):
        # 1. ç¼–ç  (Tx)
        tx_signal = self.transmitter(x)

        # 2. èƒ½é‡å½’ä¸€åŒ– (Power Normalization)
        # è¿™æ˜¯ä¸€ä¸ªç‰©ç†çº¦æŸï¼šå‘å°„æœºçš„å¹³å‡åŠŸç‡ä¸èƒ½æ— é™å¤§ï¼Œå¿…é¡»é™åˆ¶ä¸º 1
        # E[x^2] = 1
        # è®¡ç®—å½“å‰ batch çš„å¹³å‡èƒ½é‡
        # è¿™ç§å½’ä¸€åŒ–æŠ€å·§è®© AI å¿…é¡»åœ¨æœ‰é™èƒ½é‡ä¸‹ä¼˜åŒ–åˆ†å¸ƒ
        n_power = torch.mean(tx_signal ** 2)
        tx_signal = tx_signal / torch.sqrt(n_power * 2)  # *2 æ˜¯ä¸ºäº†åŒ¹é…å¤æ•°åŠŸç‡å®šä¹‰

        # 3. ç»è¿‡ä¿¡é“ (Channel)
        # åŠ é«˜æ–¯ç™½å™ªå£° (AWGN)
        noise = torch.randn_like(tx_signal) * noise_std
        rx_signal = tx_signal + noise

        # 4. è§£ç  (Rx)
        reconstructed = self.receiver(rx_signal)

        return reconstructed, tx_signal  # æŠŠä¸­é—´çš„å‘å°„ä¿¡å·ä¹Ÿè¿”å›ï¼Œæˆ‘ä»¬è¦çœ‹æ˜Ÿåº§å›¾


# --- 2. è®­ç»ƒå‡†å¤‡ ---
M = 16  # è¯•å›¾æ¨¡ä»¿ 16-QAM
model = AutoencoderComm(M=M)
optimizer = optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()  # åˆ†ç±»ä»»åŠ¡å¸¸ç”¨çš„ Loss

# å‡†å¤‡ One-hot æ•°æ® (å•ä½çŸ©é˜µå°±æ˜¯æœ€å¥½çš„ One-hot é›†åˆ)
# æ¯”å¦‚ M=4: [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]
data = torch.eye(M)

# --- 3. è®­ç»ƒå¾ªç¯ ---
print(f"ğŸš€ AI æ­£åœ¨å‘æ˜ä¸€ç§æ–°çš„ {M}-ç‚¹ è°ƒåˆ¶æ–¹å¼...")
loss_history = []

for epoch in range(2000):
    # éšæœºç”Ÿæˆä¸€æ‰¹æ¶ˆæ¯ç´¢å¼•
    batch_indices = torch.randint(0, M, (1000,))  # éšæœºé€‰ 1000 ä¸ªæ•°
    batch_inputs = data[batch_indices]  # è½¬æˆ One-hot: (1000, 16)

    # è®­ç»ƒ (è®¾ç½®ä¸€ç‚¹å™ªå£°ï¼Œé€¼è¿« AI æŠŠç‚¹æ‹‰å¼€)
    # SNR = 7dB å·¦å³
    optimizer.zero_grad()
    outputs, tx_sig = model(batch_inputs, noise_std=0.1)

    # è®¡ç®— Loss (è¾“å…¥æ˜¯ label indexï¼Œè¾“å‡ºæ˜¯æ¦‚ç‡)
    loss = criterion(outputs, batch_indices)

    loss.backward()
    optimizer.step()

    loss_history.append(loss.item())

    if epoch % 200 == 0:
        print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

# --- 4. ç»˜å›¾ï¼šAI å‘æ˜äº†ä»€ä¹ˆï¼Ÿ ---
plt.figure(figsize=(12, 5))

# å›¾1: Loss æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Epoch")

# å›¾2: AI å‘æ˜çš„æ˜Ÿåº§å›¾
plt.subplot(1, 2, 2)
# è¿™ä¸€æ­¥æˆ‘ä»¬è¦æŠŠ 16 ä¸ªæ ‡å‡†ç¬¦å·è¾“è¿›å»ï¼Œçœ‹çœ‹å®ƒæ˜ å°„åˆ°äº†å“ªé‡Œ
with torch.no_grad():
    _, constellation = model(data, noise_std=0.0)  # æ— å™ªå£°æŸ¥çœ‹
    constellation = constellation.numpy()

plt.scatter(constellation[:, 0], constellation[:, 1], c='r', s=100, marker='o')
for i in range(M):
    plt.text(constellation[i, 0] + 0.1, constellation[i, 1] + 0.1, str(i))

# ç”»ä¸ªåœ†åœˆè¡¨ç¤ºèƒ½é‡è¾¹ç•Œ
circle = plt.Circle((0, 0), 1, color='b', fill=False, linestyle='--')
plt.gca().add_patch(circle)

plt.title(f"AI-Learned Constellation (M={M})")
plt.grid(True)
plt.axis('equal')
plt.show()